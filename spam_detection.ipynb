{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Spam Detection\n",
    "\n",
    "The purpose of this notebook is to parse and classify a collection of emails as either `non-spam` or `spam` using a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read emails from file and parse into words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...complete\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# read emails from file and parse into words\n",
    "train_dir = './train-mails/'\n",
    "test_dir = './test-mails/'\n",
    "fpaths = [os.path.join(train_dir,fname) for fname in os.listdir(train_dir)]\n",
    "\n",
    "norm_words = []\n",
    "spam_words = []\n",
    "\n",
    "regex = re.compile('[A-z]{2,}')\n",
    "\n",
    "print('Reading files...', end=\"\")\n",
    "for path in fpaths:\n",
    "    with open(path) as file:\n",
    "        for i,line in enumerate(file):\n",
    "            if i == 2:\n",
    "                new_words = regex.findall(line)\n",
    "                \n",
    "                if os.path.split(path)[1][:3] == 'spm':\n",
    "                    spam_words.extend(new_words)\n",
    "                else:\n",
    "                    norm_words.extend(new_words)\n",
    "            \n",
    "            \n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create design matrix\n",
    "\n",
    "1. Generate list of 1,500 most common words\n",
    "2. Count words frequencies in each email\n",
    "3. Assign response variable (spam/non-spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_word_counts(fdir, words):\n",
    "    \n",
    "    # initialize design matrix\n",
    "    num_words = len(words)\n",
    "    X = np.zeros((len(fpaths),num_words))\n",
    "    y = np.zeros(len(fpaths))\n",
    "\n",
    "    print('Reading files...', end=\"\")\n",
    "    for i,path in enumerate(fpaths):\n",
    "        with open(path) as file:\n",
    "            for j,line in enumerate(file):\n",
    "                if j == 2:\n",
    "                    file_words = regex.findall(line)\n",
    "                    n = len(file_words)\n",
    "\n",
    "                    # record response var\n",
    "                    y[i] = os.path.split(path)[1][:3] == 'spm'\n",
    "\n",
    "                    # count words\n",
    "                    for k,word in enumerate(words):\n",
    "                        X[i,k] = file_words.count(word)/n\n",
    "                        \n",
    "    print('complete')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...complete\n",
      "Reading files...complete\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# count word frequencies and restrict to the most common\n",
    "num_words = 3000\n",
    "word_counts = Counter(norm_words + spam_words)\n",
    "word_counts = word_counts.most_common(num_words)\n",
    "common_words = [word[0] for word in word_counts]\n",
    "\n",
    "# initialize design matrices\n",
    "X_train, y_train = parse_word_counts(train_dir, common_words)\n",
    "X_test, y_test = parse_word_counts(test_dir, common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27011494252873564"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sum(axis=1).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "Train the model and test the classification accuracy on the 3000 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.86, Precision = 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "# train classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "\n",
    "# test classifier accuracy\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "print('Accuracy = %0.2f, Precision = %0.2f' % \\\n",
    "      (accuracy_score(y_test,y_pred),precision_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "The accuracy is quite high, but the data and model are slow to preprocess and train. Let's try just using the 20 most commonly appearing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...complete\n",
      "Reading files...complete\n",
      "Accuracy = 0.86, Precision = 0.93\n",
      "mail\n",
      "order\n",
      "address\n",
      "report\n",
      "language\n",
      "send\n",
      "email\n",
      "program\n",
      "our\n",
      "list\n",
      "one\n",
      "name\n",
      "money\n",
      "receive\n",
      "free\n",
      "work\n",
      "information\n",
      "business\n",
      "please\n",
      "day\n"
     ]
    }
   ],
   "source": [
    "# retrain with only the 20 most informative words\n",
    "X_train, y_train = parse_word_counts(train_dir, common_words[:20])\n",
    "X_test, y_test = parse_word_counts(test_dir, common_words[:20])\n",
    "\n",
    "# train classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "\n",
    "# test classifier accuracy\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "print('Accuracy = %0.2f, Precision = %0.2f' % \\\n",
    "      (accuracy_score(y_test,y_pred),precision_score(y_test,y_pred)))\n",
    "\n",
    "[print(word) for word in common_words[:20]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 0.86 is not bad for just picking the most commonly used words, but I suspect we can do better by choosing the 20 most differentially distributed words between spam and normal mail. We can use the KL divergence to get a score of how different the frequencies are between spam and normal mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "# find the most differentially distributed counts\n",
    "norm_x = X_train[y_train>0,:]\n",
    "spam_x = X_train[y_train==0,:]\n",
    "\n",
    "bins = np.linspace(0,.1,10)\n",
    "kl_div = np.zeros(num_words)\n",
    "\n",
    "for i in range(norm_x.shape[1]):\n",
    "    p_x = np.histogram(norm_x[:,i], bins=bins, density=True)[0]\n",
    "    q_x = np.histogram(spam_x[:,i], bins=bins, density=True)[0]\n",
    "    kl_div[i] = entropy(p_x,q_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential pitfall of this approach is that the KL divergence will go to `Inf` if our words appear in one list and not the other. For that reason, I'll restrict the resulting scores for finite KL divergence. Doing so gives us the 20 most informative words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "order\n",
      "linguistic\n",
      "university\n",
      "edu\n",
      "english\n",
      "de\n",
      "subject\n",
      "linguist\n",
      "post\n",
      "linguistics\n",
      "reference\n",
      "department\n",
      "research\n",
      "between\n",
      "anyone\n",
      "game\n",
      "grammar\n",
      "theory\n",
      "word\n"
     ]
    }
   ],
   "source": [
    "# list 20 most informative words\n",
    "sort_idx = np.argsort(kl_div)[::-1]\n",
    "sort_idx = sort_idx[~np.isinf(kl_div[sort_idx])]\n",
    "[print(common_words[i]) for i in sort_idx[:20]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...complete\n",
      "Reading files...complete\n",
      "Accuracy = 0.96, Precision = 0.94\n"
     ]
    }
   ],
   "source": [
    "# retrain with only the 20 most informative words\n",
    "most_informative_words = [common_words[i] for i in sort_idx[:20]]\n",
    "X_train, y_train = parse_word_counts(train_dir, most_informative_words)\n",
    "X_test, y_test = parse_word_counts(test_dir, most_informative_words)\n",
    "\n",
    "# train classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "\n",
    "# test classifier accuracy\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "print('Accuracy = %0.2f, Precision = %0.2f' % \\\n",
    "      (accuracy_score(y_test,y_pred),precision_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Restricting to the 20 most differentially distributed words results in a 10 pt. improvement in the accurancy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
